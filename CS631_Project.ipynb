{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUvVUhl5vlfm"
      },
      "source": [
        "## CS631 Data Intensive Distributed Computing\n",
        "### Fall 2022 - Final Project\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpjufN82vrfM"
      },
      "source": [
        "**Group member names and UW student ID numbers**\n",
        "* _Yujia Zheng (20789867)_\n",
        "* _Xiyao Wang (20704844)_\n",
        "* _KaYat Liu (20730764)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIZZBNfAwzBn"
      },
      "source": [
        "Install Spark since it is not installed in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9CUPQaymw0n4"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seJ_WhLHw1Qg"
      },
      "source": [
        "After installing Spark and Java in Colab, set the environment path which allows to run Pyspark in the Colab environment, and create SparkContext. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3mirqnFxRVs"
      },
      "source": [
        "Word embedding: TF-IDF and Word count\n",
        "\n",
        "Classification: KNN and Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IEJA9srvxMit"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "sc = SparkContext(appName=\"YourTest\", master=\"local[*]\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"ReadCSVFile\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zFgeV0YO7o6",
        "outputId": "88348eb3-b53b-4f5a-fd40-8795c3aeaea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\t       SMS_test_str.txt      SMS_training.txt\n",
            "simple_tokenize.py     SMS_test.txt\t     spark-3.2.3-bin-hadoop2.7\n",
            "SMSSpamCollection.txt  SMS_training_str.txt  spark-3.2.3-bin-hadoop2.7.tgz\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/simple_tokenize.py\n",
        "!wget -q https://git.uwaterloo.ca/y326zhen/cs631_project/-/raw/main/SMS_training.txt\n",
        "!wget -q https://git.uwaterloo.ca/y326zhen/cs631_project/-/raw/main/SMS_test.txt\n",
        "\n",
        "!wget -q https://git.uwaterloo.ca/y326zhen/cs631_project/-/raw/main/dataset/SMS_test_str.txt\n",
        "!wget -q https://git.uwaterloo.ca/y326zhen/cs631_project/-/raw/main/dataset/SMS_training_str.txt\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9pFjXfXSPTSx"
      },
      "outputs": [],
      "source": [
        "# Load the training and test datasets.\n",
        "SMS_spam_training = sc.textFile('SMS_training.txt')\n",
        "SMS_spam_test = sc.textFile('SMS_test.txt')\n",
        "SMS_training_str = sc.textFile('SMS_training_str.txt')\n",
        "SMS_test_str = sc.textFile('SMS_test_str.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SMS_training_small = SMS_training_str.take(50)\n",
        "SMS_training_small = sc.parallelize(SMS_training_small)\n",
        "SMS_test_small = SMS_test_str.take(10)\n",
        "SMS_test_small = sc.parallelize(SMS_test_small)"
      ],
      "metadata": {
        "id": "b61naRYloKDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD-x6XzbQaga"
      },
      "source": [
        "## K-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words Embedding"
      ],
      "metadata": {
        "id": "8aJna5k0U33_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-NN Bag of Words helper functions\n",
        "def get_token_features(train_data):\n",
        "\n",
        "    token_features = train_data.map(lambda line: line.split('\\t'))\\\n",
        "                            .flatMap(lambda line: simple_tokenize(line[1]))\\\n",
        "                            .map(lambda token: (token, 1))\\\n",
        "                            .reduceByKey(lambda x,y: x+y)\\\n",
        "                            .sortBy(lambda x: x[1], ascending=False)\\\n",
        "                            .keys()\n",
        "    return token_features\n",
        "\n",
        "def get_token_count_emb(token_features, data):\n",
        "\n",
        "    #token_features=get_token_features(train_data)\n",
        "    id = data.zipWithIndex().map(lambda line: line[1])\n",
        "    id_tf_cross = id.cartesian(token_features)\n",
        "    id_tf_cross = id_tf_cross.map(lambda x: (x, 0))\n",
        "\n",
        "    pairs = data.map(lambda line: line.split('\\t'))\\\n",
        "                .map(lambda line: (line[0], simple_tokenize(line[1])))\\\n",
        "                .filter(lambda x: x[1] != [])\\\n",
        "                .zipWithIndex()\\\n",
        "                .flatMap(lambda line: [((line[1], i), 1) for i in line[0][1]])\\\n",
        "                .reduceByKey(lambda x, y: x + y)\n",
        "    \n",
        "    res = id_tf_cross.leftOuterJoin(pairs)\\\n",
        "            .map(lambda x: (x[0], x[1][1]) if x[1][1] is not None else (x[0], 0))\\\n",
        "            .sortByKey()\\\n",
        "            .map(lambda x: (x[0][0], x[1]))\\\n",
        "            .groupByKey()\\\n",
        "            .mapValues(list)\n",
        "\n",
        "    return res\n",
        "\n",
        "def euc_dist(train_emb, test_emb):\n",
        "    total = 0\n",
        "    for i in range(len(train_emb)):\n",
        "        total += (train_emb[i]-test_emb[i])**2\n",
        "    return total**0.5\n",
        "\n",
        "def top_k_nearest_neighbors(test_neighbors, K):\n",
        "    test_idx = test_neighbors[0]\n",
        "    neighbors = test_neighbors[1]\n",
        "    neighbors = sorted(neighbors, key=lambda item: item[1])\n",
        "    top_k_idx = [nei[0] for nei in neighbors][:K]\n",
        "    return test_idx, top_k_idx"
      ],
      "metadata": {
        "id": "BZYzg2gZMCHg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Embedding"
      ],
      "metadata": {
        "id": "7s9Ozv6eU7bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "import math\n",
        "\n",
        "# Helper function to compute the tfidf weights for words in sentences\n",
        "def tfidf(dataset_name):\n",
        "  # tokenize the sentences, filter out empty lists after tokenize and add index\n",
        "  lines = dataset_name\n",
        "  lines_with_index = lines.map(lambda line: line.split('\\t'))\\\n",
        "                .map(lambda line: (line[0], simple_tokenize(line[1])))\\\n",
        "                .filter(lambda x: x[1] != [])\\\n",
        "                .zipWithIndex()          \n",
        "  line_counts = lines_with_index.count()\n",
        "\n",
        "  # get term frequency in a sentence for each word\n",
        "  pair = lines_with_index.flatMap(lambda line: [((line[1], i), 1) for i in line[0][1]])\\\n",
        "                .reduceByKey(lambda x, y: x + y)         \n",
        "  tf = pair.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
        "\n",
        "  # get inverse document frequency for each word in the corpus\n",
        "  idf_prep = pair.map(lambda x: (x[0][1],(x[0][0],x[1],1)))\\\n",
        "            .map(lambda x:(x[0],x[1][2]))\\\n",
        "            .reduceByKey(lambda x,y:x+y)\n",
        "  idf = idf_prep.map(lambda x: (x[0],math.log(line_counts/x[1])))\n",
        "\n",
        "  # extract tf and idf for each word and sentence index pair and compute tfidf\n",
        "  prep = tf.join(idf)\n",
        "  output = prep.map(lambda x: ((x[0], x[1][0][0]),x[1][0][1]*x[1][1])).sortByKey()\n",
        "\n",
        "  # output format: ((word, index), tfidf)                \n",
        "  return output\n",
        "\n",
        "\n",
        "# for tfidf embedding\n",
        "def get_token_features_tfidf(tf_train_data):\n",
        "\n",
        "  token_features = tf_train_data.map(lambda x: (x[0][0], x[1]))\\\n",
        "                    .reduceByKey(lambda x, y: x + y)\\\n",
        "                    .sortBy(lambda x: x[1], ascending=False)\\\n",
        "                    .keys()                     \n",
        "  # (word, freq), sorted by tfidf desc => rdd of words                       \n",
        "  return token_features\n",
        "\n",
        "\n",
        "def get_tfidf_emb(token_features, data, tfidf):\n",
        "    #token_features=get_token_features(train_data)\n",
        "    # index rdd => cartesian words (index, word), map with 0 get ((index, word), 0)\n",
        "    id = data.zipWithIndex().map(lambda line: line[1])\n",
        "    id_tf_cross = id.cartesian(token_features)\n",
        "    id_tf_cross = id_tf_cross.map(lambda x: (x, 0))\n",
        "\n",
        "    pairs = tfidf            \n",
        "    \n",
        "    # ((index, word), (0, None or count)) => (index, count) => (index, list of count)\n",
        "    res = id_tf_cross.leftOuterJoin(pairs)\\\n",
        "            .map(lambda x: (x[0], x[1][1]) if x[1][1] is not None else (x[0], 0))\\\n",
        "            .sortByKey()\\\n",
        "            .map(lambda x: (x[0][0], x[1]))\\\n",
        "            .groupByKey()\\\n",
        "            .mapValues(list)\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "iFchmA2bTrCf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "jkBpJWTHVIsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn(train_data, test_data, K, emb):\n",
        "\n",
        "    num_test = test_data.count()\n",
        "    training_labels = train_data.map(lambda line: line.split('\\t'))\\\n",
        "                                          .map(lambda line: line[0])\\\n",
        "                                          .zipWithIndex().map(lambda x: (x[1], x[0]))\n",
        "    test_labels = test_data.map(lambda line: line.split('\\t'))\\\n",
        "                                          .map(lambda line: line[0])\\\n",
        "                                          .zipWithIndex().map(lambda x: (x[1], x[0]))\n",
        "\n",
        "    if emb == 'BoW':                                      \n",
        "      # all words sorted by freq                                      \n",
        "        token_features = get_token_features(train_data)\n",
        "        train_emb = get_token_count_emb(token_features, data=train_data)\n",
        "        test_emb = get_token_count_emb(token_features, data=test_data)\n",
        "    elif emb == 'TF-IDF':\n",
        "        # all words sorted by tfidf \n",
        "        tf_idf_train =  tfidf(train_data)  \n",
        "        tf_idf_test =  tfidf(test_data)                                \n",
        "        token_features=get_token_features_tfidf(tf_idf_train)\n",
        "        train_emb = get_tfidf_emb(token_features, data=train_data, tfidf = tf_idf_train)\n",
        "        test_emb = get_tfidf_emb(token_features, data=test_data, tfidf = tf_idf_test)\n",
        "\n",
        "\n",
        "    train_test_cross = train_emb.cartesian(test_emb) # train index, emb, test index, emb\n",
        "    train_test_cross = train_test_cross.map(lambda x: (x[1][0], (x[0][0], euc_dist(x[0][1], x[1][1])))) # test index, (train index, euc dist)\n",
        "    train_test_cross = train_test_cross.groupByKey()\\\n",
        "                                      .mapValues(list)\\\n",
        "                                      .map(lambda x: top_k_nearest_neighbors(x, K))\\\n",
        "                                      .flatMap(lambda x: [(idx, x[0]) for idx in x[1]]) # selected train index, test index\n",
        "    # 1st map: => test index, train neighbors labels\n",
        "    # 2nd last map: test index, # of spams\n",
        "    preds = train_test_cross.leftOuterJoin(training_labels).map(lambda x: x[1]).groupByKey()\\\n",
        "                                      .mapValues(list)\\\n",
        "                                      .map(lambda x: (x[0], x[1].count('spam')))\\\n",
        "                                      .map(lambda x: (x[0], 'spam' if x[1]>=K/2 else 'ham'))\n",
        "    output = preds.join(test_labels)\n",
        "\n",
        "    acc = sc.accumulator(0)\n",
        "    error = sc.accumulator(0)\n",
        "    def calcutateError(x):\n",
        "        index = x[0]\n",
        "        pred_label = x[1][0]\n",
        "        true_label = x[1][1]\n",
        "        if pred_label == true_label:\n",
        "            acc.add(1)\n",
        "        else:\n",
        "            error.add(1)\n",
        "    output.foreach(calcutateError)\n",
        "\n",
        "    print(f'Test accuracy: {acc.value*100/num_test:.2f}%', \n",
        "          f'\\nTotal number of prediction errors: {error.value}')"
      ],
      "metadata": {
        "id": "Ai9pZTF1sZ7O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "knn(train_data=SMS_training_str, test_data=SMS_test_str, K=10, emb = 'BoW')"
      ],
      "metadata": {
        "id": "2wHky_Od3ZUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3aabac-5037-425f-9faa-503c617a146f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 77.26% \n",
            "Total number of prediction errors: 68\n",
            "CPU times: user 3.37 s, sys: 402 ms, total: 3.77 s\n",
            "Wall time: 9min 47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "knn(train_data=SMS_training_str, test_data=SMS_test_str, K=10, emb = 'TF-IDF')"
      ],
      "metadata": {
        "id": "cLdqP_mSHruA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5816057-367d-4c6e-d5f3-fe953e3ff712"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 50.84% \n",
            "Total number of prediction errors: 147\n",
            "CPU times: user 3.2 s, sys: 445 ms, total: 3.64 s\n",
            "Wall time: 9min 5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Incurred incredibly long runtime\n",
        "%%time\n",
        "knn(train_data=SMS_spam_training, test_data=SMS_spam_test, K=10, emb = 'BoW')"
      ],
      "metadata": {
        "id": "MFVpjK4QHeoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Incurred incredibly long runtime\n",
        "%%time\n",
        "knn(train_data=SMS_spam_training, test_data=SMS_spam_test, K=10, emb = 'TF-IDF')"
      ],
      "metadata": {
        "id": "4e0ZIi6z8X_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpgaJqgt-dGW"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words Embedding"
      ],
      "metadata": {
        "id": "CsS6cO7RUKWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "from simple_tokenize import simple_tokenize\n",
        "import math\n",
        "\n",
        "# Helper function to compute the bag of words embedding\n",
        "def bag_of_words(dataset_name):\n",
        "  # tokenize the sentences, filter out empty lists after tokenize and add index\n",
        "  lines_with_index = dataset_name.map(lambda line: line.split('\\t'))\\\n",
        "                .map(lambda line: (line[0], simple_tokenize(line[1])))\\\n",
        "                .filter(lambda x: x[1] != [])\\\n",
        "                .zipWithIndex()          \n",
        "  line_counts = lines_with_index.count()\n",
        "\n",
        "  # get word count in a sentence for each word\n",
        "  pair = lines_with_index.flatMap(lambda line: [((line[1], i), 1) for i in line[0][1]])\\\n",
        "                .reduceByKey(lambda x, y: x + y)         \n",
        "  word_count = pair.map(lambda x: ((x[0][1], x[0][0]), x[1]))\n",
        "  \n",
        "  return word_count"
      ],
      "metadata": {
        "id": "8T0a9p0gUCiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Embedding"
      ],
      "metadata": {
        "id": "sZDSlfsWDBkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to compute the tfidf weights for words in sentences\n",
        "def tfidf(dataset_name):\n",
        "  # tokenize the sentences, filter out empty lists after tokenize and add index\n",
        "  lines = dataset_name\n",
        "  lines_with_index = lines.map(lambda line: line.split('\\t'))\\\n",
        "                .map(lambda line: (line[0], simple_tokenize(line[1])))\\\n",
        "                .filter(lambda x: x[1] != [])\\\n",
        "                .zipWithIndex()          \n",
        "  line_counts = lines_with_index.count()\n",
        "\n",
        "  # get term frequency in a sentence for each word\n",
        "  pair = lines_with_index.flatMap(lambda line: [((line[1], i), 1) for i in line[0][1]])\\\n",
        "                .reduceByKey(lambda x, y: x + y)         \n",
        "  tf = pair.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
        "\n",
        "  # get inverse document frequency for each word in the corpus\n",
        "  idf_prep = pair.map(lambda x: (x[0][1],(x[0][0],x[1],1)))\\\n",
        "            .map(lambda x:(x[0],x[1][2]))\\\n",
        "            .reduceByKey(lambda x,y:x+y)\n",
        "  idf = idf_prep.map(lambda x: (x[0],math.log(line_counts/x[1])))\n",
        "\n",
        "  # extract tf and idf for each word and sentence index pair and compute tfidf\n",
        "  prep = tf.join(idf)\n",
        "  output = prep.map(lambda x: ((x[0], x[1][0][0]),x[1][0][1]*x[1][1])).sortByKey()\n",
        "\n",
        "  # output format: ((word, index), tfidf)                \n",
        "  return output\n"
      ],
      "metadata": {
        "id": "o4LBr55BCvWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "9GRKJK9_VMP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function builds the Naive Bayes classifier with tfidf scores\n",
        "def train(train_dataset, emb_func):\n",
        "  # tokenize the sentences, filter out empty lists after tokenize and add index\n",
        "  lines = train_dataset\n",
        "  lines_with_index = lines.map(lambda line: line.split('\\t'))\\\n",
        "                .map(lambda line: (line[0], simple_tokenize(line[1])))\\\n",
        "                .filter(lambda x: x[1] != [])\\\n",
        "                .zipWithIndex()         \n",
        "  line_counts = lines_with_index.count()\n",
        "\n",
        "  # get prior for each class\n",
        "  label = lines_with_index.map(lambda x: (x[1], x[0][0]))\n",
        "  ham_counts = label.filter(lambda x: x[1] == 'ham').count()\n",
        "  spam_counts = label.filter(lambda x: x[1] == 'spam').count()\n",
        "  ham_prob = ham_counts/line_counts\n",
        "  spam_prob = spam_counts/line_counts\n",
        "  distinct_label = label.map(lambda x: x[1]).distinct()\n",
        "\n",
        "  # setup for computing conditional probabilities\n",
        "  words = lines.map(lambda line: line.split('\\t')).flatMap(lambda line: simple_tokenize(line[1])).distinct()\n",
        "  word_counts = words.count()\n",
        "  all_words = words.map(lambda x: (x, 0))\n",
        "  cross_prep = distinct_label.cartesian(all_words).map(lambda x: ((x[0], x[1][0]), x[1][1]))\n",
        "  dataset = emb_func(train_dataset)\n",
        "  \n",
        "  # get the label for the sentence and sum of tfidf for each (label, word) combination\n",
        "  dataset_by_label = dataset.map(lambda x: (x[0][1], (x[0][0], x[1])))\\\n",
        "                            .join(label)\\\n",
        "                            .map(lambda x: ((x[1][1], x[1][0][0]), x[1][0][1]))\\\n",
        "                            .reduceByKey(lambda x, y: x + y)\n",
        "  # sum of tfidf for each class\n",
        "  total_by_label = dataset_by_label.map(lambda x: (x[0][0], x[1])).reduceByKey(lambda x, y: x + y)\n",
        "  total_by_label = total_by_label.collectAsMap() # small table\n",
        "  broadcast_tbl = sc.broadcast(total_by_label)\n",
        "\n",
        "  # map the cross product of (label, word) combination with the existing (label, word) table\n",
        "  # and assign tfidf from existing table or 0 otherwise\n",
        "  # then calculate probability \n",
        "  raw_prob_dataset = cross_prep.leftOuterJoin(dataset_by_label)\n",
        "  def getProb(x):\n",
        "    label = x[0][0]\n",
        "    word = x[0][1]\n",
        "    default_zero = x[1][0]\n",
        "    value_to_check = x[1][1]\n",
        "    if value_to_check == None:\n",
        "      emb = 0\n",
        "    else:\n",
        "      emb = value_to_check\n",
        "    prob = (emb + 1)/(broadcast_tbl.value[label] + word_counts)\n",
        "    return ((label, word), prob)\n",
        "  prob_dataset = raw_prob_dataset.map(getProb)\n",
        "\n",
        "  # output format for dataset: ((label, word), prob)\n",
        "  return prob_dataset, ham_prob, spam_prob, distinct_label\n"
      ],
      "metadata": {
        "id": "ygxXZ8UkdYmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function takes a test dataset and a training dataset and returns the \n",
        "# accuracy of the test set after training using training set\n",
        "def test(train_dataset, dataset_name, emb_func):\n",
        "  # tokenize the sentences, filter out empty lists after tokenize and add index, extract true label for comparision\n",
        "  lines = dataset_name\n",
        "  lines_with_index = lines.map(lambda line: line.split('\\t'))\\\n",
        "                .map(lambda line: (line[0], simple_tokenize(line[1])))\\\n",
        "                .filter(lambda x: x[1] != [])\\\n",
        "                .zipWithIndex()            \n",
        "  line_counts = lines_with_index.count()\n",
        "  label = lines_with_index.map(lambda x: (x[1], x[0][0]))\n",
        "\n",
        "  # get prior from training set\n",
        "  ham_prob = train(train_dataset, emb_func)[1]\n",
        "  spam_prob = train(train_dataset, emb_func)[2]\n",
        "  distinct_label = train(train_dataset, emb_func)[3]\n",
        "\n",
        "  # get (label, word) combination for each sentence and for all classes\n",
        "  # and extract probability from training set\n",
        "  # if word does not exist in training set, assign a small probability to it\n",
        "  # then multiply the conditional probability as well as the prior for each sentence\n",
        "  def createIndexWord(x):\n",
        "    label = x[0][0]\n",
        "    list_of_words = x[0][1]\n",
        "    index = x[1]\n",
        "    output = []\n",
        "    for word in list_of_words:\n",
        "      output.append((word, index))\n",
        "    return output\n",
        "  prep_for_label = lines_with_index.flatMap(createIndexWord)\n",
        "  cross_prep = distinct_label.cartesian(prep_for_label).map(lambda x: ((x[0], x[1][0]), x[1][1]))\n",
        "  prob_dataset_dict = train(train_dataset, emb_func)[0]\n",
        "  full_table = cross_prep.leftOuterJoin(prob_dataset_dict)\n",
        "\n",
        "  def getProbTrain(x):\n",
        "    label = x[0][0]\n",
        "    word = x[0][1]\n",
        "    index = x[1][0]\n",
        "    prob_to_check = x[1][1]\n",
        "    if prob_to_check == None:\n",
        "      return ((index, label), 0.0001) # if does not exist in training, assign 0.0001\n",
        "    else:\n",
        "      return ((index, label), prob_to_check)\n",
        "  test_dataset = full_table.map(getProbTrain).reduceByKey(lambda x, y: x * y)\n",
        "\n",
        "  def get_prior(x):\n",
        "    index = x[0][0]\n",
        "    label = x[0][1]\n",
        "    prob = x[1]\n",
        "    if label == 'ham':\n",
        "      prior = ham_prob\n",
        "    else:\n",
        "      prior = spam_prob\n",
        "    return ((index, label), prob * prior)\n",
        "  test_dataset = test_dataset.map(get_prior)\n",
        "\n",
        "  # get predicting label for each sentence and compare with true label to calculate error and accuracy\n",
        "  def max_label(a, b):\n",
        "    prob_a = a[1]\n",
        "    prob_b = b[1]\n",
        "    if prob_a < prob_b:\n",
        "      return b  \n",
        "    else:\n",
        "      return a\n",
        "  test_dataset = test_dataset.map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
        "  test_dataset = test_dataset.reduceByKey(max_label).map(lambda x: (x[0], x[1][0])) \n",
        "  output = test_dataset.join(label)\n",
        "\n",
        "  acc = sc.accumulator(0)\n",
        "  error = sc.accumulator(0)\n",
        "  def calcutateError(x):\n",
        "    index = x[0]\n",
        "    pred_label = x[1][0]\n",
        "    true_label = x[1][1]\n",
        "    if pred_label == true_label:\n",
        "      acc.add(1)\n",
        "    else:\n",
        "      error.add(1)\n",
        "  output.foreach(calcutateError)\n",
        "\n",
        "  print(f'Test accuracy: {acc.value*100/line_counts:.2f}%', \n",
        "        f'\\nTotal number of prediction errors: {error.value}')\n",
        "  return"
      ],
      "metadata": {
        "id": "4eFm5bbFdaDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(SMS_spam_training, SMS_spam_test, emb_func=bag_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DisUAkNhkzGK",
        "outputId": "34f91abb-9931-4809-9544-d395fff00a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 98.39% \n",
            "Total number of prediction errors: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(SMS_training_str, SMS_test_str, emb_func=bag_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4Yt_EiQlAxB",
        "outputId": "dd034712-4f3c-42fe-c4f0-8d1aba50f243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 95.32% \n",
            "Total number of prediction errors: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(SMS_spam_training, SMS_spam_test, emb_func=tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR6ylyZbkfyt",
        "outputId": "881890fc-ac15-4af0-e7a8-9602242ad1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 98.57% \n",
            "Total number of prediction errors: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(SMS_training_str, SMS_test_str, emb_func=tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4SN4wlalJRC",
        "outputId": "f87ef169-4b91-488d-ed27-e720cd5ff9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 95.99% \n",
            "Total number of prediction errors: 12\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}